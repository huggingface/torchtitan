[1mdiff --git a/tt_run.log.filtered b/hf_run.log.filtered[m
[1mindex d3be70f..0f9a180 100644[m
[1m--- a/tt_run.log.filtered[m
[1m+++ b/hf_run.log.filtered[m
[36m@@ -1,22 +1,23 @@[m
+ echo [31m'##############################################'[m
[31m##############################################[m[32m'#######################################################'[m
[32m#######################################################[m
+ echo '### Running TorchTitan [31m(native)[m[32mwith HF backend[m training ###'
### Running TorchTitan [31m(native)[m[32mwith HF backend[m training ###
+ echo [31m'##############################################'[m
[31m##############################################[m[32m'#######################################################'[m
[32m#######################################################[m
+ [31mTT_CONFIG=/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_tt.toml[m[32mHF_CONFIG=/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_hf.toml[m
+ [31mCUDA_VISIBLE_DEVICES=0[m[32mCUDA_VISIBLE_DEVICES=1[m
+ torchrun ... --master_port=XXXX --rdzv_backend c10d --rdzv_endpoint=localhost:XXXX --local-ranks-filter 0 --role rank --tee 3 -m torchtitan.train --job.config_file [31m/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_tt.toml[m[32m/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_hf.toml[m --training.seed 42 --training.deterministic
[rank0]:/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/transformers/src/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.[m
[rank0]:  warnings.warn([m
[rank0]:[titan] TIMESTAMP - root - [32mWARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.[m
[32m[rank0]:[titan] TIMESTAMP - root -[m INFO - Starting job: [32mHF[m Llama 3 debug training
[rank0]:[titan] TIMESTAMP - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config[m
[rank0]:[titan] TIMESTAMP - root - INFO - Building 0-D device mesh with [], [][m
[rank0]:[titan] TIMESTAMP - root - INFO - [GC] Initial GC collection 0.00 seconds[m
[rank0]:[titan] TIMESTAMP - root - INFO - Deterministic algorithm enabled (expect perf degradation).[m
[rank0]:[titan] TIMESTAMP - root - INFO - Loading tokenizer from tokenizer.json[m
[rank0]:[titan] TIMESTAMP - root - INFO - Preparing c4_test dataset from /fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/tests/assets/c4_test[m
[rank0]:[titan] TIMESTAMP - root - INFO - Building [31mllama3[m[32mmeta-llama/Llama-3.2-1B[m debugmodel with [31mTransformerModelArgs(_enforced='This[m[32mHFTransformerModelArgs(_enforced='This[m field is used to enforce all fields have [31mdefaults.', dim=256, n_layers=6, n_heads=16, n_kv_heads=None, vocab_size=2000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)[m[32mdefaults.')[m
[rank0]:[titan] TIMESTAMP - root - INFO - CUDA capacity: NVIDIA H100 80GB HBM3 with 79.44GiB memory[m
[rank0]:[titan] TIMESTAMP - root - INFO - Parameter breakdown:[m
[rank0]:[titan] TIMESTAMP - root - INFO -   - embedding: 512,000 parameters[m
[36m@@ -28,30 +29,29 @@[m
[rank0]:[titan] TIMESTAMP - root - INFO -   - layer_5: 852,480 parameters[m
[rank0]:[titan] TIMESTAMP - root - INFO -   - final_norm: 256 parameters[m
[rank0]:[titan] TIMESTAMP - root - INFO -   - lm_head: 512,000 parameters[m
[rank0]:[titan] TIMESTAMP - root - INFO - [34mModel [31mllama3[m[32mmeta-llama/Llama-3.2-1B[m debugmodel [31msize: 6,139,136 total parameters[39m
[rank0]:[titan] TIMESTAMP - root - INFO - Applied selective activation checkpointing to the model[m
[rank0]:[titan] TIMESTAMP - root - INFO - Peak FLOPS used for computing MFU: 9.890e+14[m
[rank0]:[titan] TIMESTAMP - root - INFO - CUDA memory usage for model: 0.04GiB(0.05%)[m
[31m[rank0]:[titan] TIMESTAMP - root - WARNING - model.safetensors.index.json not found at hf_assets_path: /fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/tests/assets/tokenizer/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format[m
[rank0]:[titan] TIMESTAMP - root - INFO - Mixed precision training is handled by AMP[m
[rank0]:[titan] TIMESTAMP - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2)[m
[rank0]:[titan] TIMESTAMP - root - INFO - Training starts at step 1[m
[rank0]:[titan] TIMESTAMP - root - INFO - Profiling active. Traces will be saved at [31m./outputs/profile_trace[m[32m./outputs/profile_trace_hf[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  1  [32mloss:  [31m7.8723[m[32m7.8704[m  [38;2;180;60;0mgrad_norm:  [31m1.5167[m[32m1.5185[m  [38;2;54;234;195mmemory:  [31m1.39GiB(1.75%)[m[32m1.67GiB(2.10%)[m  [34mtps: [31m44,585[m[32m34,083[m  [36mtflops: [31m3.19[m[32m2.54[m  [35mmfu: [31m0.32%[39m[m[32m0.26%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  2  [32mloss:  [31m7.5246[m[32m7.5209[m  [38;2;180;60;0mgrad_norm:  [31m1.6359[m[32m1.6373[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m21,052[m[32m19,870[m  [36mtflops: [31m1.51[m[32m1.48[m  [35mmfu: 0.15%[39m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  3  [32mloss:  [31m6.7900[m[32m6.7789[m  [38;2;180;60;0mgrad_norm:  [31m2.0345[m[32m2.0390[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m334,947[m[32m199,616[m  [36mtflops: [31m23.95[m[32m14.89[m  [35mmfu: [31m2.42%[39m[m[32m1.51%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  4  [32mloss:  [31m5.9829[m[32m5.9673[m  [38;2;180;60;0mgrad_norm:  [31m2.4129[m[32m2.4176[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m357,001[m[32m207,967[m  [36mtflops: [31m25.53[m[32m15.51[m  [35mmfu: [31m2.58%[39m[m[32m1.57%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  5  [32mloss:  [31m5.0536[m[32m5.0388[m  [38;2;180;60;0mgrad_norm:  [31m2.5305[m[32m2.5275[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m279,263[m[32m188,745[m  [36mtflops: [31m19.97[m[32m14.08[m  [35mmfu: [31m2.02%[39m[m[32m1.42%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Dumping profiler traces at step 5[m
[rank0]:[titan] TIMESTAMP - root - INFO - Finished dumping profiler traces in [31m0.02[m[32m0.04[m seconds
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  6  [32mloss:  [31m4.6370[m[32m4.6283[m  [38;2;180;60;0mgrad_norm:  [31m2.2826[m[32m2.2818[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m129,464[m[32m83,088[m  [36mtflops: [31m9.26[m[32m6.20[m  [35mmfu: [31m0.94%[39m[m[32m0.63%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  7  [32mloss:  [31m4.3133[m[32m4.3077[m  [38;2;180;60;0mgrad_norm:  [31m2.1019[m[32m2.1023[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m298,394[m[32m175,561[m  [36mtflops: [31m21.34[m[32m13.09[m  [35mmfu: [31m2.16%[39m[m[32m1.32%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  8  [32mloss:  [31m4.1398[m[32m4.1349[m  [38;2;180;60;0mgrad_norm:  [31m1.9342[m[32m1.9334[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m352,929[m[32m206,086[m  [36mtflops: [31m25.24[m[32m15.37[m  [35mmfu: [31m2.55%[39m[m[32m1.55%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  9  [32mloss:  [31m4.5326[m[32m4.5289[m  [38;2;180;60;0mgrad_norm:  [31m1.5111[m[32m1.5103[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m357,192[m[32m208,947[m  [36mtflops: [31m25.54[m[32m15.58[m  [35mmfu: [31m2.58%[39m[m[32m1.58%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep: 10  [32mloss:  [31m3.9859[m[32m3.9828[m  [38;2;180;60;0mgrad_norm:  [31m1.7799[m[32m1.7849[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m287,408[m[32m189,593[m  [36mtflops: [31m20.55[m[32m14.14[m  [35mmfu: [31m2.08%[39m[m[32m1.43%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Dumping profiler traces at step 10[m
[rank0]:[titan] TIMESTAMP - root - INFO - Finished dumping profiler traces in [31m0.03[m[32m0.04[m seconds
[rank0]:[titan] TIMESTAMP - root - INFO - Sleeping 2 seconds for other ranks to complete[m
[rank0]:[titan] TIMESTAMP - root - INFO - Training completed[m
[rank0]:[titan] TIMESTAMP - root - INFO - Process group destroyed[m
