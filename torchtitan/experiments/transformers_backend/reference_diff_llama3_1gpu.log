[1mdiff --git a/tt_run.log.filtered b/hf_run.log.filtered[m
[1mindex 28327e0..abbe4d7 100644[m
[1m--- a/tt_run.log.filtered[m
[1m+++ b/hf_run.log.filtered[m
[36m@@ -1,125 +1,125 @@[m
+ echo [31m'##############################################'[m
[31m##############################################[m[32m'#######################################################'[m
[32m#######################################################[m
+ echo '### Running TorchTitan [31m(native)[m[32mwith HF backend[m training ###'
### Running TorchTitan [31m(native)[m[32mwith HF backend[m training ###
+ echo [31m'##############################################'[m
[31m##############################################[m[32m'#######################################################'[m
[32m#######################################################[m
+ [31mTT_CONFIG=/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_tt.toml[m[32mHF_CONFIG=/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_hf.toml[m
+ [31mCUDA_VISIBLE_DEVICES=0[m[32mCUDA_VISIBLE_DEVICES=1[m
+ torchrun ... --master_port=XXXX --rdzv_backend c10d --rdzv_endpoint=localhost:XXXX --local-ranks-filter 0 --role rank --tee 3 -m torchtitan.train --job.config_file [31m/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_tt.toml[m[32m/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/torchtitan/experiments/transformers_backend/configs/debug_1_gpu_hf.toml[m --training.seed 42 --training.deterministic --model.name [31mllama3[m[32mmeta-llama/Llama-3.2-1B[m
[rank0]:/fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/transformers/src/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.[m
[rank0]:  warnings.warn([m
[rank0]:[titan] TIMESTAMP - root - [32mWARNING - tokenizer_path is deprecated, use model.hf_assets_path instead. Setting hf_assets_path to tokenizer_path temporarily.[m
[32m[rank0]:[titan] TIMESTAMP - root -[m INFO - Starting job: [32mHF[m Llama 3 debug training
[rank0]:[titan] TIMESTAMP - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config[m
[rank0]:[titan] TIMESTAMP - root - INFO - Building 0-D device mesh with [], [][m
[rank0]:[titan] TIMESTAMP - root - INFO - [GC] Initial GC collection 0.00 seconds[m
[rank0]:[titan] TIMESTAMP - root - INFO - Deterministic algorithm enabled (expect perf degradation).[m
[rank0]:[titan] TIMESTAMP - root - INFO - Loading tokenizer from tokenizer.json[m
[rank0]:[titan] TIMESTAMP - root - INFO - Preparing c4_test dataset from /fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/tests/assets/c4_test[m
[rank0]:[titan] TIMESTAMP - root - INFO - Building [31mllama3[m[32mmeta-llama/Llama-3.2-1B[m debugmodel with [31mTransformerModelArgs(_enforced='This[m[32mHFTransformerModelArgs(_enforced='This[m field is used to enforce all fields have [31mdefaults.', dim=256, n_layers=6, n_heads=16, n_kv_heads=None, vocab_size=2000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=500000, max_seq_len=2048, depth_init=True, use_flex_attn=False, attn_mask_type='causal', eos_id=0)[m[32mdefaults.')[m
[rank0]:[titan] TIMESTAMP - root - INFO - CUDA capacity: NVIDIA H100 80GB HBM3 with 79.44GiB memory[m
[rank0]:[titan] TIMESTAMP - root - INFO - Model Structure Parameter Breakdown:[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mTransformer[m[32mHFTransformerModel[m - 6,139,136 params
[rank0]:[titan] TIMESTAMP - root - INFO -   [31m(tok_embeddings):[m[32m(embed_tokens):[m Embedding - 512,000 params
[rank0]:[titan] TIMESTAMP - root - INFO -   (layers): [31mModuleDict[m[32mModuleList[m - 5,114,880 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (0): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (1): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (2): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (3): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (4): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -     (5): [31mTransformerBlock[m[32mLlamaDecoderLayer[m - 852,480 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention): Attention[m[32m(self_attn): LlamaAttention[m - 262,144 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wq):[m[32m(q_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wk):[m[32m(k_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wv):[m[32m(v_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(wo):[m[32m(o_proj):[m Linear - 65,536 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(feed_forward): FeedForward[m[32m(mlp): LlamaMLP[m - 589,824 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w1):[m[32m(gate_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w2):[m[32m(up_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -         [31m(w3):[m[32m(down_proj):[m Linear - 196,608 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(attention_norm): RMSNorm[m[32m(input_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -       [31m(ffn_norm): RMSNorm[m[32m(post_attention_layernorm): LlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -   (norm): [31mRMSNorm[m[32mLlamaRMSNorm[m - 256 params
[rank0]:[titan] TIMESTAMP - root - INFO -   [31m(output):[m[32m(lm_head):[m Linear - 512,000 params
[rank0]:[titan] TIMESTAMP - root - INFO - [34mModel [31mllama3[m[32mmeta-llama/Llama-3.2-1B[m debugmodel [31msize: 6,139,136 total parameters[39m
[rank0]:[titan] TIMESTAMP - root - INFO - Applied selective activation checkpointing to the model[m
[rank0]:[titan] TIMESTAMP - root - INFO - Peak FLOPS used for computing MFU: 9.890e+14[m
[rank0]:[titan] TIMESTAMP - root - INFO - CUDA memory usage for model: 0.04GiB(0.05%)[m
[31m[rank0]:[titan] TIMESTAMP - root - WARNING - model.safetensors.index.json not found at hf_assets_path: /fsx/ferdinandmom/ferdinand-hf/huggingface/torchtitan/tests/assets/tokenizer/model.safetensors.index.json.                     Defaulting to saving a single safetensors file if checkpoint is saved in HF format[m
[rank0]:[titan] TIMESTAMP - root - INFO - Mixed precision training is handled by AMP[m
[rank0]:[titan] TIMESTAMP - root - INFO - Trainer is initialized with local batch size 8, global batch size 8, gradient accumulation steps 1, sequence length 2048, total steps 10 (warmup 2)[m
[rank0]:[titan] TIMESTAMP - root - INFO - Training starts at step 1[m
[rank0]:[titan] TIMESTAMP - root - INFO - Profiling active. Traces will be saved at [31m./outputs/profile_trace[m[32m./outputs/profile_trace_hf[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  1  [32mloss:  [31m7.8723[m[32m7.8704[m  [38;2;180;60;0mgrad_norm:  [31m1.5167[m[32m1.5185[m  [38;2;54;234;195mmemory:  [31m1.39GiB(1.75%)[m[32m1.67GiB(2.10%)[m  [34mtps: [31m43,792[m[32m34,528[m  [36mtflops: [31m3.13[m[32m2.58[m  [35mmfu: [31m0.32%[39m[m[32m0.26%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  2  [32mloss:  [31m7.5246[m[32m7.5209[m  [38;2;180;60;0mgrad_norm:  [31m1.6359[m[32m1.6373[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m21,384[m[32m19,712[m  [36mtflops: [31m1.53[m[32m1.47[m  [35mmfu: 0.15%[39m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  3  [32mloss:  [31m6.7900[m[32m6.7789[m  [38;2;180;60;0mgrad_norm:  [31m2.0345[m[32m2.0390[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m336,714[m[32m197,260[m  [36mtflops: [31m24.08[m[32m14.71[m  [35mmfu: [31m2.43%[39m[m[32m1.49%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  4  [32mloss:  [31m5.9829[m[32m5.9673[m  [38;2;180;60;0mgrad_norm:  [31m2.4129[m[32m2.4176[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m360,388[m[32m206,932[m  [36mtflops: [31m25.77[m[32m15.43[m  [35mmfu: [31m2.61%[39m[m[32m1.56%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  5  [32mloss:  [31m5.0536[m[32m5.0388[m  [38;2;180;60;0mgrad_norm:  [31m2.5305[m[32m2.5275[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m286,298[m[32m186,563[m  [36mtflops: [31m20.47[m[32m13.91[m  [35mmfu: [31m2.07%[39m[m[32m1.41%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Dumping profiler traces at step 5[m
[rank0]:[titan] TIMESTAMP - root - INFO - Finished dumping profiler traces in [31m0.03[m[32m0.05[m seconds
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  6  [32mloss:  [31m4.6370[m[32m4.6283[m  [38;2;180;60;0mgrad_norm:  [31m2.2826[m[32m2.2818[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m129,447[m[32m80,608[m  [36mtflops: [31m9.26[m[32m6.01[m  [35mmfu: [31m0.94%[39m[m[32m0.61%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  7  [32mloss:  [31m4.3133[m[32m4.3077[m  [38;2;180;60;0mgrad_norm:  [31m2.1019[m[32m2.1023[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m300,058[m[32m177,619[m  [36mtflops: [31m21.46[m[32m13.25[m  [35mmfu: [31m2.17%[39m[m[32m1.34%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  8  [32mloss:  [31m4.1398[m[32m4.1349[m  [38;2;180;60;0mgrad_norm:  [31m1.9342[m[32m1.9334[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m361,523[m[32m205,777[m  [36mtflops: [31m25.85[m[32m15.35[m  [35mmfu: [31m2.61%[39m[m[32m1.55%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep:  9  [32mloss:  [31m4.5326[m[32m4.5289[m  [38;2;180;60;0mgrad_norm:  [31m1.5111[m[32m1.5103[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m363,412[m[32m207,933[m  [36mtflops: [31m25.99[m[32m15.51[m  [35mmfu: [31m2.63%[39m[m[32m1.57%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - [31mstep: 10  [32mloss:  [31m3.9859[m[32m3.9828[m  [38;2;180;60;0mgrad_norm:  [31m1.7799[m[32m1.7849[m  [38;2;54;234;195mmemory:  [31m1.52GiB(1.91%)[m[32m1.79GiB(2.26%)[m  [34mtps: [31m295,360[m[32m188,228[m  [36mtflops: [31m21.12[m[32m14.04[m  [35mmfu: [31m2.14%[39m[m[32m1.42%[39m[m
[rank0]:[titan] TIMESTAMP - root - INFO - Dumping profiler traces at step 10[m
[rank0]:[titan] TIMESTAMP - root - INFO - Finished dumping profiler traces in [31m0.03[m[32m0.04[m seconds
[rank0]:[titan] TIMESTAMP - root - INFO - Sleeping 2 seconds for other ranks to complete[m
[rank0]:[titan] TIMESTAMP - root - INFO - Training completed[m
[rank0]:[titan] TIMESTAMP - root - INFO - Process group destroyed[m
